{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e00cc99",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AK spruce beetle outbreak risk pipeline\n",
    "\n",
    "This notebook constitutes the pipeline for producing a dataset of projected climate-driven risk of spruce beetle outbreak for forested areas of Alaska for the 21st century. See the [README](README.md) for more information.\n",
    "\n",
    "### Outputs\n",
    "\n",
    "The main product of this pipeline is a 5-D datacube of one categorical variable - climate-driven spruce beetle outbreak risk. The dimensions are:  \n",
    "\n",
    "* Era (time period)\n",
    "* Model\n",
    "* Scenario\n",
    "* Snowpack level\n",
    "* Y\n",
    "* X\n",
    "\n",
    "##### Format / structure\n",
    "\n",
    "This will be realized in typical SNAP / ARDAC fashion: a set of GeoTIFFs containing risk values for the entire spatial domain for a single realization of the first four dimension values, i.e. coordinates, and named according to those unique coordinate combinations.\n",
    "\n",
    "##### Spatial extent\n",
    "\n",
    "The expected spatial extent of the final dataset is the extent of the forest layer that the final risk data will be masked to. This will come from a version of the binary USFS \"Alaska Forest/Non-forest Map\" raster (found [here](https://data.fs.usda.gov/geodata/rastergateway/biomass/alaska_forest_nonforest.php)) in SNAP holdings that has been reprojected to EPSG:3338, found at `/workspace/Shared/Tech_Projects/beetles/project_data/ak_forest_mask.tif`.\n",
    "\n",
    "##### Temporal extent\n",
    "\n",
    "The risk values will be computed for 30-year long eras of the 21st century:  \n",
    "* 2010-2039\n",
    "* 2040-2069\n",
    "* 2070-2099\n",
    "\n",
    "### Base data\n",
    "\n",
    "The base / input data used for computing the climate-driven risk of beetle outbreaks is the \"[21st Century Hydrologic Projections for Alaska and Hawaii](https://www.earthsystemgrid.org/dataset/ucar.ral.hydro.predictions.html)\" dataset produced by NCAR, specifically the \"Alaska Near Surface Meteorology Daily Averages\" child dataset. This dataset is available on SNAP infra at `/Data/Base_Data/Climate/AK_NCAR_12km/met`.\n",
    "\n",
    "## Pipeline steps\n",
    "\n",
    "0. Setup - Set up path variables, slurm variables, directories, intial conditions, etc. Execute the setup code cell before any other step\n",
    "1. Process yearly risk components - the risk model\n",
    "\n",
    "### 0 - Setup\n",
    "\n",
    "Set up path variables, slurm variables, directories, intial conditions, etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db789067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import compute_yearly_risk as main\n",
    "import slurm\n",
    "\n",
    "\n",
    "ncar_dir = Path(os.getenv(\"AK_NCAR_DIR\"))\n",
    "base_dir = Path(os.getenv(\"BASE_DIR\"))\n",
    "output_dir = Path(os.getenv(\"OUTPUT_DIR\"))\n",
    "scratch_dir = Path(os.getenv(\"SCRATCH_DIR\"))\n",
    "conda_init_script = Path(os.getenv(\"CONDA_INIT\"))\n",
    "project_dir = Path(os.getenv(\"PROJECT_DIR\"))\n",
    "# binary directory from the current conda environment is appended to PATH\n",
    "path_str = os.getenv(\"PATH\")\n",
    "# can use this to activate the anaconda-project env\n",
    "ap_env = Path(path_str.split(\":\")[0]).parent\n",
    "\n",
    "# met_dir = Path(\"/Data/Base_Data/Climate/AK_NCAR_12km/met\")\n",
    "# path to the input meteorological dataset\n",
    "met_dir = ncar_dir.joinpath(\"met\")\n",
    "\n",
    "tmp_fn = \"{}_{}_BCSD_met_{}.nc4\"\n",
    "\n",
    "# path to directory where risk components datasets will be written\n",
    "risk_comp_dir = scratch_dir.joinpath(\"risk_components\")\n",
    "risk_comp_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# path to directory where yearly risk datasets will be written\n",
    "yearly_risk_dir = scratch_dir.joinpath(\"yearly_risk\")\n",
    "yearly_risk_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# daymet_comp_fp = scratch_dir.joinpath(\"yearly_risk_components_daymet.nc\")\n",
    "# path to directory where slurm scripts (jobs and outputs) will be written\n",
    "slurm_dir = scratch_dir.joinpath(\"slurm\")\n",
    "slurm_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "\n",
    "# \n",
    "# risk_script = \"/workspace/UA/kmredilla/spruce-beetle-risk/compute_yearly_risk.py\"\n",
    "risk_script = project_dir.joinpath(\"compute_yearly_risk.py\")\n",
    "\n",
    "# init script for conda on compute nodes\n",
    "\n",
    "slurm_email = \"kmredilla@alaska.edu\"\n",
    "partition = \"main\"\n",
    "# conda_init_script = \"/home/UA/kmredilla/conda_init.sh\"\n",
    "# conda_env_name = \"py39\"\n",
    "ncpus = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261733d5-cefa-487a-88ba-848742906d8d",
   "metadata": {},
   "source": [
    "## Process Daymet yearly risk components dataset\n",
    "\n",
    "Create a dataset of the yearly risk component values as a useful precursor to the synthesized overall risk value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec5e1aad-930d-4b09-9e35-d0ada7fd146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_tmp_fn = \"{}_met_{}.nc\"\n",
    "dm_era = \"1980-2017\"\n",
    "dm_model = \"daymet\"\n",
    "dm_ncpus = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13eb3041-19b8-4c34-baac-7752a6cd18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "daymet_risk_comp_ds = main.process_risk_components(\n",
    "    met_dir, dm_tmp_fn, dm_era, dm_model, dm_ncpus\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81a7bf21-2c36-40ea-9c30-8cd655b01b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "daymet_comp_fp = scratch_dir.joinpath(\"yearly_risk_components_daymet.nc\")\n",
    "\n",
    "daymet_risk_comp_ds.to_netcdf(daymet_comp_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d777290-cb91-4074-ae58-ce81c73da792",
   "metadata": {},
   "source": [
    "## Process Daymet yearly risk dataset\n",
    "\n",
    "Derive the yearly risk dataset from the components dataset.\n",
    "\n",
    "We are working with 1980-2017, and a single year's risk computation requires two years' worth of data prior. So our first valid year will be 1982."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "508622e4-6d69-4850-834d-1e508bd2b278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "const_args = {\"model\": \"daymet\", \"scenario\": None}\n",
    "snow_values = [\"low\", \"med\", \"high\"]\n",
    "\n",
    "yearly_risk_arrs = []\n",
    "\n",
    "with xr.open_dataset(daymet_comp_fp) as comp_ds:\n",
    "    for year in range(1982, 2018):\n",
    "        u_t2 = comp_ds[\"summer_survival\"].sel(year=(year - 2), **const_args).values\n",
    "        u_t1 = comp_ds[\"summer_survival\"].sel(year=(year - 1), **const_args).values\n",
    "        # \"not univoltine\"\n",
    "        un_t2 = np.round(1 - u_t2, 2)\n",
    "        x2_t2 = comp_ds[\"fall_survival\"].sel(year=(year - 2), **const_args).values\n",
    "        x2_t1 = comp_ds[\"fall_survival\"].sel(year=(year - 1), **const_args).values\n",
    "\n",
    "        year_snow_risk = []\n",
    "        for snow in snow_values:\n",
    "            x3_t2 = comp_ds[\"winter_survival\"].sel(year=(year - 2), snow=snow, **const_args).values\n",
    "            x3_t1 = comp_ds[\"winter_survival\"].sel(year=(year - 1), snow=snow, **const_args).values\n",
    "\n",
    "            # original equation\n",
    "            # (un_t2 * sv_p * x2_t2 * x2_t1 * x3_t2 * x3_t1) + ((u_t2 * p * x2_t2 * x3_t2) * (u_t1 * p * x2_t1 * x3_t1)) + (u_t2 * p * x2_t2 * x2_t1 * x3_t2 * x3_t1)\n",
    "            # simplified algebra\n",
    "            year_snow_risk.append(main.compute_risk(u_t1, u_t2, un_t2, x2_t1, x2_t2, x3_t1, x3_t2))\n",
    "\n",
    "        yearly_risk_arrs.append(np.array(year_snow_risk))\n",
    "    \n",
    "yearly_risk_arr = np.swapaxes(np.array(yearly_risk_arrs), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5243e03-bcef-4fc2-b85b-21190f8974f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "\n",
    "daymet_risk_ds = xr.Dataset(\n",
    "    # need to expand dims to add an extra for each of model, scenario\n",
    "    data_vars={\"risk\": ([\"snow\", \"year\", \"y\", \"x\"], yearly_risk_arr)},\n",
    "    coords={\n",
    "        \"year\": ([\"year\"], np.arange(1982, 2018)),\n",
    "        \"longitude\": ([\"y\", \"x\"], comp_ds[\"longitude\"].values),\n",
    "        \"latitude\": ([\"y\", \"x\"], comp_ds[\"latitude\"].values),\n",
    "        \"snow\": ([\"snow\"], snow_values),\n",
    "    },\n",
    "    attrs=dict(description=\"Climate-based beetle risk\",),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6daddcf-9964-4f3d-ab67-9dbb4bc2da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "daymet_risk_fp = \"/workspace/Shared/Tech_Projects/beetles/final_products/yearly_risk_daymet.nc\"\n",
    "\n",
    "daymet_risk_ds.to_netcdf(daymet_risk_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf11d73-e76f-465e-9ddb-011963abab25",
   "metadata": {},
   "source": [
    "## Process CMIP5 yearly risk components dataset\n",
    "\n",
    "Create a dataset of the yearly risk component values as a useful precursor to the synthesized overall risk value for a single CMIP5 model.\n",
    "\n",
    "Do this for a far future period, for RCP85, for GFDL-ESM2M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69a52390-8e4b-4574-99aa-d08899a64bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "era = \"2068-2099\"\n",
    "model = \"HadGEM2-ES\"\n",
    "scenario = \"rcp85\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a906d00-7d0b-4f40-abea-68fea617cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "era = \"2068-2099\"\n",
    "model = \"GFDL-ESM2M\"\n",
    "scenario = \"rcp85\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c507dddd-ebe1-47f2-a3e2-62ceeded013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_comp_ds = main.process_risk_components(\n",
    "    met_dir, tmp_fn, era, model, ncpus, scenario\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7abbe59c-1e16-4449-af99-c2cda678f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_fp = scratch_dir.joinpath(f\"yearly_risk_components_{model}_{scenario}.nc\")\n",
    "\n",
    "risk_comp_ds.to_netcdf(comp_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8061c09e-4b4d-47df-8fbc-dfba1baffd89",
   "metadata": {},
   "source": [
    "## Process CMIP5 yearly risk dataset\n",
    "\n",
    "Derive the yearly risk dataset from the components dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2d198e3-7ef5-4302-8cc2-921a0bbcf93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "const_args = {\"model\": model, \"scenario\": scenario}\n",
    "snow_values = [\"low\", \"med\", \"high\"]\n",
    "\n",
    "yearly_risk_arrs = []\n",
    "\n",
    "with xr.open_dataset(comp_fp) as comp_ds:\n",
    "    for year in range(2070, 2100):\n",
    "        u_t2 = comp_ds[\"summer_survival\"].sel(year=(year - 2), **const_args).values\n",
    "        u_t1 = comp_ds[\"summer_survival\"].sel(year=(year - 1), **const_args).values\n",
    "        # \"not univoltine\"\n",
    "        un_t2 = np.round(1 - u_t2, 2)\n",
    "        x2_t2 = comp_ds[\"fall_survival\"].sel(year=(year - 2), **const_args).values\n",
    "        x2_t1 = comp_ds[\"fall_survival\"].sel(year=(year - 1), **const_args).values\n",
    "\n",
    "        year_snow_risk = []\n",
    "        for snow in snow_values:\n",
    "            x3_t2 = comp_ds[\"winter_survival\"].sel(year=(year - 2), snow=snow, **const_args).values\n",
    "            x3_t1 = comp_ds[\"winter_survival\"].sel(year=(year - 1), snow=snow, **const_args).values\n",
    "\n",
    "            # original equation\n",
    "            year_snow_risk.append(main.compute_risk(u_t1, u_t2, un_t2, x2_t1, x2_t2, x3_t1, x3_t2))\n",
    "\n",
    "        yearly_risk_arrs.append(np.array(year_snow_risk))\n",
    "    \n",
    "yearly_risk_arr = np.swapaxes(np.array(yearly_risk_arrs), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42350044-4042-4e3b-9153-71100b124145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "\n",
    "risk_ds = xr.Dataset(\n",
    "    # need to expand dims to add an extra for each of model, scenario\n",
    "    data_vars={\"risk\": ([\"snow\", \"year\", \"y\", \"x\"], yearly_risk_arr)},\n",
    "    coords={\n",
    "        \"year\": ([\"year\"], np.arange(2070, 2100)),\n",
    "        \"longitude\": ([\"y\", \"x\"], comp_ds[\"longitude\"].values),\n",
    "        \"latitude\": ([\"y\", \"x\"], comp_ds[\"latitude\"].values),\n",
    "        \"snow\": ([\"snow\"], snow_values),\n",
    "    },\n",
    "    attrs=dict(description=\"Climate-based beetle risk\",),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6460a4e3-e570-48e6-b4fb-2775c525d3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_fp = f\"/workspace/Shared/Tech_Projects/beetles/final_products/yearly_risk_{model}_{scenario}.nc\"\n",
    "\n",
    "risk_ds.to_netcdf(risk_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4083875c-d3ca-43a2-bbac-44ac341d8856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867de55b-abf4-46e7-837d-e4bd3ad8eff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d461564-583e-42a8-9f8d-7b0eb263e937",
   "metadata": {},
   "source": [
    "## 1 - Process yearly risk\n",
    "\n",
    "This section creates the yearly risk dataset - a collection of risk values for each year across the grid. This dataset is not expected to be the final product, but it could be a useful intermediate product.\n",
    "\n",
    "### 1.1 - Process yearly risk components\n",
    "\n",
    "The yearly \"risk components\" dataset may have some merit on its own, at least for validation if nothing else. This is the three main variables derived from a year's climate data which are necessary for calculating the risk for any given year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3626ac64-4f77-4714-b50a-d248aabcbf1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'slurm' from '/workspace/UA/kmredilla/spruce-beetle-risk/slurm.py'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "importlib.reload(slurm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13578c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm /atlas_scratch/kmredilla/beetles/slurm/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15191eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all projections will have years 2010-2099\n",
    "# need to start with 2008 as yearly risk calculation\n",
    "#   requires risk components from two years prior\n",
    "era = \"2008-2099\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69ac598b-a5a0-4d73-a51d-a8cff5e6255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = \"GFDL-ESM2M\"\n",
    "scenario = \"rcp85\"\n",
    "sbatch_fp, sbatch_out_fp = slurm.get_yearly_fps(slurm_dir, model, era, scenario)\n",
    "risk_comp_fp = risk_comp_dir.joinpath(f\"risk_components_{model}_{scenario}_{era}.nc\")\n",
    "yearly_risk_fp = yearly_risk_dir.joinpath(f\"yearly_risk_{model}_{scenario}_{era}.nc\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2929f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import luts\n",
    "\n",
    "\n",
    "sbatch_fps = []\n",
    "for model in luts.models:\n",
    "    for scenario in luts.scenarios:\n",
    "        sbatch_fp, sbatch_out_fp = slurm.get_yearly_fps(slurm_dir, model, era, scenario)\n",
    "        risk_comp_fp = risk_comp_dir.joinpath(f\"risk_components_{model}_{scenario}_{era}.nc\")\n",
    "        yearly_risk_fp = yearly_risk_dir.joinpath(f\"yearly_risk_{model}_{scenario}_{era}.nc\")\n",
    "        \n",
    "        kwargs = {\n",
    "            \"slurm_email\": slurm_email,\n",
    "            \"partition\": partition,\n",
    "            \"conda_init_script\": conda_init_script,\n",
    "            \"ap_env\": ap_env,\n",
    "            \"sbatch_fp\": sbatch_fp,\n",
    "            \"sbatch_out_fp\": sbatch_out_fp,\n",
    "            \"risk_script\": risk_script,\n",
    "            \"met_dir\": met_dir,\n",
    "            \"tmp_fn\": tmp_fn,\n",
    "            \"risk_comp_fp\": risk_comp_fp,\n",
    "            \"yearly_risk_fp\": yearly_risk_fp,\n",
    "            \"era\": era,\n",
    "            \"model\": model,\n",
    "            \"scenario\": scenario,\n",
    "        }\n",
    "\n",
    "        slurm.write_sbatch_yearly_risk(**kwargs)\n",
    "        sbatch_fps.append(sbatch_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2af05b",
   "metadata": {},
   "source": [
    "Remove existing slurm output files if desired:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e088ea4f-93f1-42d1-a169-e16db4b9bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove existing output files if desired\n",
    "_ = [fp.unlink() for fp in slurm_dir.glob(\"*.out\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fd5dc7",
   "metadata": {},
   "source": [
    "Submit the sbatch jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20db0341",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ids = [slurm.submit_sbatch(fp) for fp in sbatch_fps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0008bbc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49653f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc6bb49b-664f-4936-8396-cef2627ffd56",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Process CMIP5 yearly risk dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e565168-ce9f-4683-9625-64913a7b9683",
   "metadata": {},
   "source": [
    "Create a slurm sbatch script for each model, scenario, and era combination. Each job will occupy a node to read the data in parallel and return a risk dataset. Define a function to create this script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e178e43c-59d9-4479-bf1e-fa046d5ebc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sbatch_yearly_risk(\n",
    "    slurm_email,\n",
    "    partition,\n",
    "    conta_init_script,\n",
    "    conda_env_name,\n",
    "    sbatch_fp,\n",
    "    sbatch_out_fp,\n",
    "    compute_yearly_risk,\n",
    "    met_dir,\n",
    "    tmp_fn,\n",
    "    era,\n",
    "    model,\n",
    "    scenario,\n",
    "    ncpus,\n",
    "    risk_fp\n",
    "):\n",
    "    sbatch_head = (\n",
    "        \"#!/bin/sh\\n\"\n",
    "        \"#SBATCH --nodes=1\\n\"\n",
    "        \"#SBATCH --cpus-per-task={}\\n\"\n",
    "        \"#SBATCH --mail-type=FAIL\\n\"\n",
    "        f\"#SBATCH --mail-user={slurm_email}\\n\"\n",
    "        f\"#SBATCH -p {partition}\\n\"\n",
    "        \"#SBATCH --output {}\\n\"\n",
    "        # print start time\n",
    "        \"echo Start slurm && date\\n\"\n",
    "        # prepare shell for using activate - Chinook requirement\n",
    "        f\"source {conda_init_script}\\n\"\n",
    "        f\"conda activate {conda_env_name}\\n\"\n",
    "    )\n",
    "\n",
    "    pycommands = \"\\n\"\n",
    "    pycommands += (\n",
    "        f\"python {compute_yearly_risk} \"\n",
    "        f\"--met_dir {met_dir} \"\n",
    "        f\"--tmp_fn {tmp_fn} \"\n",
    "        f\"--era {era} \"\n",
    "        f\"--model {model} \"\n",
    "        f\"--scenario {scenario} \"\n",
    "        f\"--ncpus {ncpus} \"\n",
    "        f\"--risk_fp {risk_fp}\\n\\n\"\n",
    "    )\n",
    "    commands = sbatch_head.format(ncpus, sbatch_out_fp) + pycommands\n",
    "\n",
    "    with open(sbatch_fp, \"w\") as f:\n",
    "        f.write(commands)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091f0f84-8ced-4850-990d-f6fb0fb8558d",
   "metadata": {},
   "source": [
    "Build sbatch files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a9c66f-a7ae-410c-bc0c-ba17c0816f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import luts\n",
    "\n",
    "\n",
    "sbatch_fps = []\n",
    "risk_fps = []\n",
    "for model in luts.models:\n",
    "    for scenario in luts.scenarios:\n",
    "        for era in luts.eras:\n",
    "            if era in [\"2040-2099\"]:\n",
    "                continue\n",
    "            sbatch_fp = slurm_dir.joinpath(\n",
    "                f\"yearly_risk_{model}_{scenario}_{era}.slurm\"\n",
    "            )\n",
    "            sbatch_out_fp = str(sbatch_fp).replace(\".slurm\", \"_%j.out\")\n",
    "            # temporary filepath for yearly data array\n",
    "            risk_fp = scratch_dir.joinpath(f\"{model}_{scenario}_{era}.nc\")\n",
    "            write_sbatch_yearly_risk(\n",
    "                slurm_email,\n",
    "                partition,\n",
    "                conda_init_script,\n",
    "                conda_env_name,\n",
    "                sbatch_fp,\n",
    "                sbatch_out_fp,\n",
    "                compute_yearly_risk,\n",
    "                met_dir,\n",
    "                tmp_fn,\n",
    "                era,\n",
    "                model,\n",
    "                scenario,\n",
    "                ncpus,\n",
    "                risk_fp\n",
    "            )\n",
    "            sbatch_fps.append(sbatch_fp)\n",
    "            risk_fps.append(risk_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e74f083a-3179-4037-b941-71dedcdded7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove existing output files if desired\n",
    "_ = [fp.unlink() for fp in slurm_dir.glob(\"*.out\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb676ab4-feae-4abf-b12b-eea972aed34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "def submit_sbatch(sbatch_fp):\n",
    "    \"\"\"Submit a script to slurm via sbatch\n",
    "    \n",
    "    Args:\n",
    "        sbatch_fp (pathlib.PosixPath): path to .slurm script to submit\n",
    "        \n",
    "    Returns:\n",
    "        job id for submitted job\n",
    "    \"\"\"\n",
    "    out = subprocess.check_output([\"sbatch\", str(sbatch_fp)])\n",
    "    job_id = out.decode().replace(\"\\n\", \"\").split(\" \")[-1]\n",
    "\n",
    "    return job_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f1ed7a-4fc9-4b05-a65d-4c1fd28d16e5",
   "metadata": {},
   "source": [
    "Submit the sbatch jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a3caa3e-fb15-47df-9bcf-4d68402d4a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ids = [submit_sbatch(fp) for fp in sbatch_fps]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352c8e76-6198-4c3c-a8cc-118af46f0898",
   "metadata": {},
   "source": [
    "Read in all temporary DataArrays and combine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "723905ca-7cdc-4c43-8212-66fb72249a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "\n",
    "risk_da = xr.combine_by_coords([xr.open_dataarray(fp) for fp in risk_fps])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6417148b-fb29-4b75-8d3d-a24364aa0c3a",
   "metadata": {},
   "source": [
    "Save to a single file on Poseidon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9db428bf-e6b2-454c-97fa-75a425e73ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fp = \"/workspace/Shared/Tech_Projects/beetles/final_products/yearly_risk.nc\"\n",
    "\n",
    "risk_da.to_netcdf(out_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0dd012-00f2-4f12-9a21-116050f36d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
