{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e00cc99",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AK spruce beetle outbreak risk pipeline\n",
    "\n",
    "This notebook constitutes the pipeline for producing a dataset of projected climate-driven risk of spruce beetle outbreak for forested areas of Alaska for the 21st century. See the [README](README.md) for more information.\n",
    "\n",
    "### Outputs\n",
    "\n",
    "The main product of this pipeline is a 5-D datacube of one categorical variable - climate-driven spruce beetle outbreak risk. The dimensions are:  \n",
    "\n",
    "* Era (time period)\n",
    "* Model\n",
    "* Scenario\n",
    "* Snowpack level\n",
    "* Y\n",
    "* X\n",
    "\n",
    "##### Format / structure\n",
    "\n",
    "This will be realized in typical SNAP / ARDAC fashion: a set of GeoTIFFs containing risk values for the entire spatial domain for a single realization of the first four dimension values, i.e. coordinates, and named according to those unique coordinate combinations.\n",
    "\n",
    "##### Spatial extent\n",
    "\n",
    "The expected spatial extent of the final dataset is the extent of the forest layer that the final risk data will be masked to. This will come from a version of the binary USFS \"Alaska Forest/Non-forest Map\" raster (found [here](https://data.fs.usda.gov/geodata/rastergateway/biomass/alaska_forest_nonforest.php)) in SNAP holdings that has been reprojected to EPSG:3338, found at `/workspace/Shared/Tech_Projects/beetles/project_data/ak_forest_mask.tif`.\n",
    "\n",
    "##### Temporal extent\n",
    "\n",
    "The risk values will be computed for 30-year long eras of the 21st century:  \n",
    "* 2010-2039\n",
    "* 2040-2069\n",
    "* 2070-2099\n",
    "\n",
    "### Base data\n",
    "\n",
    "The base / input data used for computing the climate-driven risk of beetle outbreaks is the \"[21st Century Hydrologic Projections for Alaska and Hawaii](https://www.earthsystemgrid.org/dataset/ucar.ral.hydro.predictions.html)\" dataset produced by NCAR, specifically the \"Alaska Near Surface Meteorology Daily Averages\" child dataset. This dataset is available on SNAP infra at `/Data/Base_Data/Climate/AK_NCAR_12km/met`.\n",
    "\n",
    "## Pipeline steps\n",
    "\n",
    "0. Setup - Set up path variables, slurm variables, directories, intial conditions, etc.\n",
    "1. Process yearly risk and risk components\n",
    "2. Process the final risk class dataset\n",
    "\n",
    "## 0 - Setup\n",
    "\n",
    "Set up path variables, slurm variables, directories, intial conditions, etc. Execute this cell before any other step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "db789067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import compute_yearly_risk as main\n",
    "import slurm\n",
    "\n",
    "\n",
    "ncar_dir = Path(os.getenv(\"AK_NCAR_DIR\"))\n",
    "base_dir = Path(os.getenv(\"BASE_DIR\"))\n",
    "output_dir = Path(os.getenv(\"OUTPUT_DIR\"))\n",
    "scratch_dir = Path(os.getenv(\"SCRATCH_DIR\"))\n",
    "conda_init_script = Path(os.getenv(\"CONDA_INIT\"))\n",
    "project_dir = Path(os.getenv(\"PROJECT_DIR\"))\n",
    "# binary directory from the current conda environment is appended to PATH\n",
    "path_str = os.getenv(\"PATH\")\n",
    "# can use this to activate the anaconda-project env\n",
    "ap_env = Path(path_str.split(\":\")[0]).parent\n",
    "\n",
    "# met_dir = Path(\"/Data/Base_Data/Climate/AK_NCAR_12km/met\")\n",
    "# path to the input meteorological dataset\n",
    "met_dir = ncar_dir.joinpath(\"met\")\n",
    "\n",
    "# path to directory where risk components datasets will be written\n",
    "risk_comp_dir = scratch_dir.joinpath(\"risk_components\")\n",
    "risk_comp_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# directory where yearly risk datasets will be written\n",
    "yearly_risk_dir = scratch_dir.joinpath(\"yearly_risk\")\n",
    "yearly_risk_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# directory where risk class dataset will be written\n",
    "risk_class_dir = scratch_dir.joinpath(\"risk_class\")\n",
    "risk_class_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# output direcotry for risk class\n",
    "out_risk_dir = output_dir.joinpath(\"risk_class\")\n",
    "out_risk_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# daymet_comp_fp = scratch_dir.joinpath(\"yearly_risk_components_daymet.nc\")\n",
    "# path to directory where slurm scripts (jobs and outputs) will be written\n",
    "slurm_dir = scratch_dir.joinpath(\"slurm\")\n",
    "slurm_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# script for computing yearly risk for a subset of the data\n",
    "risk_script = project_dir.joinpath(\"compute_yearly_risk.py\")\n",
    "\n",
    "# Forest mask of Alaska in EPSG:3338\n",
    "forest_fp = base_dir.joinpath(\"ak_forest_mask.tif\")\n",
    "\n",
    "\n",
    "slurm_email = \"kmredilla@alaska.edu\"\n",
    "partition = \"main\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d461564-583e-42a8-9f8d-7b0eb263e937",
   "metadata": {},
   "source": [
    "## 1 - Process yearly risk and risk components\n",
    "\n",
    "This section creates the yearly risk dataset - a collection of risk values for each year across the grid. This dataset is not expected to be the final product, but it could be a useful intermediate product.\n",
    "\n",
    "The yearly risk values are calculated from three yearly \"risk components\". Saving these components as a dataset may have some merit on its own, at least for validation if nothing else. This step utilizes slurm to handle execution of the `compute_yearly_risk.py` script on all model/scenario combinations. \n",
    "\n",
    "We will process all future years for the expected final summary time periods: 2008-2099. We will process all years available for the Daymet dataset as well: 1980-2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13578c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm /atlas_scratch/kmredilla/beetles/slurm/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92e6bcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /atlas_scratch/kmredilla/beetles/slurm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15191eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all projections will have years 2010-2099\n",
    "# need to start with 2008 as yearly risk calculation\n",
    "#   requires risk components from two years prior\n",
    "full_future_era = \"2008-2099\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2929f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import luts\n",
    "\n",
    "\n",
    "kwargs = {\n",
    "    \"slurm_email\": slurm_email,\n",
    "    \"partition\": partition,\n",
    "    \"conda_init_script\": conda_init_script,\n",
    "    \"ap_env\": ap_env,\n",
    "    \"sbatch_fp\": sbatch_fp,\n",
    "    \"sbatch_out_fp\": sbatch_out_fp,\n",
    "    \"risk_script\": risk_script,\n",
    "    \"met_dir\": met_dir,\n",
    "    # template filename for NCAR met data\n",
    "    \"tmp_fn\": \"{}_{}_BCSD_met_{}.nc4\",\n",
    "}\n",
    "\n",
    "sbatch_fps = []\n",
    "for model in luts.models:\n",
    "    for scenario in luts.scenarios:\n",
    "        sbatch_fp, sbatch_out_fp = slurm.get_yearly_fps(slurm_dir, model, full_future_era, scenario)\n",
    "        risk_comp_fp = risk_comp_dir.joinpath(f\"risk_components_{model}_{scenario}_{full_future_era}.nc\")\n",
    "        yearly_risk_fp = yearly_risk_dir.joinpath(f\"yearly_risk_{model}_{scenario}_{full_future_era}.nc\")\n",
    "        \n",
    "        kwargs.update({\n",
    "            \"sbatch_fp\": sbatch_fp,\n",
    "            \"sbatch_out_fp\": sbatch_out_fp,\n",
    "            \"risk_comp_fp\": risk_comp_fp,\n",
    "            \"yearly_risk_fp\": yearly_risk_fp,\n",
    "            \"era\": era,\n",
    "            \"model\": model,\n",
    "            \"scenario\": scenario,\n",
    "        })\n",
    "\n",
    "        slurm.write_sbatch_yearly_risk(**kwargs)\n",
    "        sbatch_fps.append(sbatch_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c18164b",
   "metadata": {},
   "source": [
    "We also have the daymet dataset that needs to be processed using different years from all of the projected data. Create an sbatch job for that, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9555f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"daymet\"\n",
    "era = \"1980-2017\"\n",
    "sbatch_fp, sbatch_out_fp = slurm.get_yearly_fps(slurm_dir, model, era)\n",
    "risk_comp_fp = risk_comp_dir.joinpath(f\"risk_components_{model}_{era}.nc\")\n",
    "yearly_risk_fp = yearly_risk_dir.joinpath(f\"yearly_risk_{model}_{era}.nc\")\n",
    "\n",
    "kwargs.update({\n",
    "    \"sbatch_fp\": sbatch_fp,\n",
    "    \"sbatch_out_fp\": sbatch_out_fp,\n",
    "    \"met_dir\": met_dir,\n",
    "    \"tmp_fn\": \"daymet_met_{}.nc\",\n",
    "    \"risk_comp_fp\": risk_comp_fp,\n",
    "    \"yearly_risk_fp\": yearly_risk_fp,\n",
    "    \"era\": era,\n",
    "    \"model\": model,\n",
    "    \"scenario\": None,\n",
    "})\n",
    "\n",
    "slurm.write_sbatch_yearly_risk(**kwargs)\n",
    "sbatch_fps.append(sbatch_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2af05b",
   "metadata": {},
   "source": [
    "Remove existing slurm output files if desired:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e088ea4f-93f1-42d1-a169-e16db4b9bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove existing output files if desired\n",
    "_ = [fp.unlink() for fp in slurm_dir.glob(\"*.out\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fd5dc7",
   "metadata": {},
   "source": [
    "Submit the sbatch jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20db0341",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ids = [slurm.submit_sbatch(fp) for fp in sbatch_fps]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82461ee",
   "metadata": {},
   "source": [
    "## 2 - Process the final risk class dataset\n",
    "\n",
    "Process the yearly risk data into risk classes for the three future eras. Since this doesn't take very long, we can process in the notebook instead of slurming it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107d0d66",
   "metadata": {},
   "source": [
    "### 2.1 - Prepare files for masking risk class dataset\n",
    "\n",
    "We want the final risk class dataset to be masked to the forested areas of Alaska, so there is some prep work that needs to happen first:\n",
    "\n",
    "1. Georeference the NCAR grid and save for a template for regridding the forest mask\n",
    "2. Re-grid the forest mask (~250m resolution) to match the NCAR template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fefdea5",
   "metadata": {},
   "source": [
    "#### 2.1.1 Georeference NCAR grid\n",
    "\n",
    "The NCAR data files have only the latitude and longitude geogrids defining the centerpoints of each pixel in the grid - no other spatial information. This is therefore the case for our new risk components and yearly risk datasets. \n",
    "\n",
    "To mask our grid to the forested area of Alaska, we want a forest mask raster that is on the same grid as our new datasets, which is the same grid as our expected grid for our risk classes dataset.\n",
    "\n",
    "So we want to create a GeoTIFF file for the NCAR grid as a template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "883fbe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "from wrf import PolarStereographic\n",
    "from pyproj import Proj, Transformer\n",
    "\n",
    "\n",
    "# open an NCAR file to get some info from\n",
    "with xr.open_dataset(met_dir.joinpath(\"daymet/daymet_met_1980.nc\")) as ds:\n",
    "    # need grid shape below\n",
    "    ny, nx = ds.longitude.shape\n",
    "    ncar_arr = np.flipud(ds[\"tmin\"].values[0])\n",
    "\n",
    "# values provided by NCAR (via email correspondence)\n",
    "wrf_proj_str = PolarStereographic(**{\"TRUELAT1\": 64, \"STAND_LON\": -150}).proj4()\n",
    "wrf_proj = Proj(wrf_proj_str)\n",
    "wgs_proj = Proj(proj='latlong', datum='WGS84')\n",
    "transformer = Transformer.from_proj(wgs_proj, wrf_proj)\n",
    "e, n = transformer.transform(-150, 64)\n",
    "# Grid parameters\n",
    "dx, dy = 12000, 12000\n",
    "# Down left corner of the domain\n",
    "x0 = -(nx-1) / 2. * dx + e\n",
    "y0 = -(ny-1) / 2. * dy + n\n",
    "# 2d grid\n",
    "x = np.arange(nx) * dx + x0\n",
    "y = np.arange(ny) * dy + y0\n",
    "\n",
    "da = xr.DataArray(\n",
    "    data=ncar_arr,\n",
    "    dims=[\"y\", \"x\"],\n",
    "    coords=dict(\n",
    "        y=([\"y\"], np.flip(y)),\n",
    "        x=([\"x\"], x),\n",
    "    ),\n",
    ")\n",
    "da.attrs[\"_FillValue\"] = np.nan\n",
    "\n",
    "temp_ncar_fp = scratch_dir.joinpath(\"ncar_template_3338.tif\")\n",
    "da.rio.write_crs(wrf_proj_str).rio.reproject(\"EPSG:3338\").rio.to_raster(temp_ncar_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995291dc",
   "metadata": {},
   "source": [
    "#### 2.1.2 - Regrid the forest mask\n",
    "\n",
    "Now regrid the forest mask to match the new NCAR template. \n",
    "\n",
    "Since the NCAR data has a larger extent than the forest mask, we will clip (crop) the template file to the extent of the forest mask before regridding the mask.\n",
    "\n",
    "Create a shapefile to clip to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9f185680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new index file...\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "\n",
    "cut_fp = scratch_dir.joinpath(\"clip_ncar.shp\")\n",
    "cut_fp.unlink(missing_ok=True)\n",
    "_ = subprocess.call([\"gdaltindex\", cut_fp, forest_fp])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aab9dda",
   "metadata": {},
   "source": [
    "Then clip the template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4676f921",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ncar_clip_fp = scratch_dir.joinpath(\"ncar_template_clipped_3338.tif\")\n",
    "temp_ncar_clip_fp.unlink(missing_ok=True)\n",
    "_ = subprocess.call(\n",
    "    [\n",
    "        \"gdalwarp\",\n",
    "        \"-cutline\",\n",
    "        cut_fp,\n",
    "        \"-crop_to_cutline\",\n",
    "        \"-q\",\n",
    "        \"-overwrite\",\n",
    "        temp_ncar_fp,\n",
    "        temp_ncar_clip_fp,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bd6816",
   "metadata": {},
   "source": [
    "Then get the new metadata from the clipped NCAR file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3617b197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio as rio\n",
    "\n",
    "\n",
    "with rio.open(temp_ncar_clip_fp) as src:\n",
    "    temp_meta = src.meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a9ddfe",
   "metadata": {},
   "source": [
    "Update the data type and nodata value to match that of existing mask: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8a2ca2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_meta.update({\"dtype\": \"uint8\", \"nodata\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a76d2",
   "metadata": {},
   "source": [
    "Write a blank array with this metadata to a new GeoTIFF that will serve as a target grid for the original forest mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c7762e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_arr = np.zeros((1, temp_meta[\"height\"], temp_meta[\"width\"]), dtype=\"uint8\")\n",
    "\n",
    "ncar_forest_fp = scratch_dir.joinpath(\"ak_forest_mask_ncar_3338.tif\")\n",
    "ncar_forest_fp.unlink(missing_ok=True)\n",
    "with rio.open(ncar_forest_fp, \"w\", **temp_meta) as src:\n",
    "    src.write(temp_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee48763",
   "metadata": {},
   "source": [
    "Now regrid the original forest mask by calling `gadalwarp` on it with this new target GeoTIFF as the output file. The data of the target file will be updated to match the original file, effectively regridding the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "4ad33c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = subprocess.call([\"gdalwarp\", \"-q\", forest_fp, ncar_forest_fp])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ab65b8",
   "metadata": {},
   "source": [
    "### 2.2 - Classify risk and mask\n",
    "\n",
    "Using our new forest mask and template NCAR file for clipping, we will classify risk, clip, and mask all output GeoTIFFs.\n",
    "\n",
    "Define the function for classifying risk from the numerical yearly risk values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "03c7c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_risk(arr):\n",
    "    \"\"\"Classify an array of risk values as being either\n",
    "    low, medium, or high, encoded as 1, 2, or 3, respectively.\n",
    "    \n",
    "    Args:\n",
    "        arr (numpy.ndarray): a 1-D array of risk values\n",
    "        \n",
    "    Returns:\n",
    "        risk_class (int): risk class, either 1, 2, or 3\n",
    "    \"\"\"\n",
    "    # this should only be used on 1-D arrays representing yearly\n",
    "    #  risk at a single pixel\n",
    "    assert(len(arr.shape) == 1)\n",
    "    \n",
    "    # risk classes are based on whether number of years over some\n",
    "    #  threshold is greater than half of total years\n",
    "    half = arr.shape[0] / 2\n",
    "    \n",
    "    # high and medium risk thresholds\n",
    "    high_thr = 0.24\n",
    "    med_thr = 0.12\n",
    "    \n",
    "    if any(np.isnan(arr)):\n",
    "        risk_class = 0\n",
    "    elif (arr >= high_thr).sum() >= half:\n",
    "        risk_class = 3\n",
    "    elif (arr >= med_thr).sum() >= half:\n",
    "        risk_class = 2\n",
    "    else:\n",
    "        risk_class = 1\n",
    "        \n",
    "    return risk_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d817eb2",
   "metadata": {},
   "source": [
    "Then iterate over the models / scenarios / snow levels / future eras, classify, clip, and mask: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "10ed0dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "args = product(luts.models, luts.scenarios, [\"low\", \"med\"], luts.eras)\n",
    "\n",
    "for model, scenario, snow, era in args:\n",
    "    yearly_risk_fp = yearly_risk_dir.joinpath(f\"yearly_risk_{model}_{scenario}_{full_future_era}.nc\")\n",
    "    start_year, end_year = [int(year) for year in era.split(\"-\")]\n",
    "    year_sl = slice(start_year, end_year)\n",
    "    with xr.open_dataset(yearly_risk_fp) as ds:\n",
    "        era_risk_arr = ds[\"risk\"].sel(year=slice(start_year, end_year), snow=snow).values\n",
    "\n",
    "    risk_class_arr = np.apply_along_axis(\n",
    "        classify_risk,\n",
    "        0,\n",
    "        ds[\"risk\"].sel(snow=snow, year=year_sl).values,\n",
    "    ).astype(np.uint8)\n",
    "\n",
    "    # create an xarray.DataArray for writing risk class\n",
    "    #  data to georeferenced file\n",
    "    da = xr.DataArray(\n",
    "        data=risk_class_arr,\n",
    "        dims=[\"y\", \"x\"],\n",
    "        coords=dict(\n",
    "            y=([\"y\"], np.flip(y)),\n",
    "            x=([\"x\"], x),\n",
    "        ),\n",
    "        attrs={\"_FillValue\": 0}\n",
    "    )\n",
    "\n",
    "    # write to a temporary file for clipping with gdal\n",
    "    tmp_class_fp = risk_class_dir.joinpath(f\"risk_class_{era}_{model}_{scenario}_{snow}.tif\")\n",
    "    da.rio.write_crs(wrf_proj_str).rio.reproject(\"EPSG:3338\").rio.to_raster(tmp_class_fp)\n",
    "    # run the clip\n",
    "    class_clip_fp = utils.clip_with_gdal(tmp_class_fp, cut_fp)\n",
    "    # remove the temporary file\n",
    "    tmp_class_fp.unlink()\n",
    "    # Then mask with forest and re-write\n",
    "    with rio.open(class_clip_fp, \"r+\") as src:\n",
    "        with rio.open(ncar_forest_fp) as mask_src:\n",
    "            arr = src.read(1)\n",
    "            mask = mask_src.read(1).astype(bool)\n",
    "            arr[~mask] = 0\n",
    "            src.write(arr, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f082ff15",
   "metadata": {},
   "source": [
    "Rename these files to get rid of the \"_clip\" suffix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "5ca7ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [\n",
    "    fp.rename(fp.parent.joinpath(fp.name.replace(\"_clip.tif\", \".tif\")))\n",
    "    for fp in risk_class_dir.glob(\"*.tif\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100df840",
   "metadata": {},
   "source": [
    "And copy these files to `$OUTPUT_DIR` for safe-keeping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "becc0a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "copy_args = [(fp, out_risk_dir.joinpath(fp.name)) for fp in risk_class_dir.glob(\"*.tif\")]\n",
    "_ = [shutil.copy(*arg) for arg in copy_args]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9664581a",
   "metadata": {},
   "source": [
    "## Pipeline end!\n",
    "\n",
    "That's it! Beetle risk secured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "a72a6c2d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "risk_class_2010-2039_CCSM4_rcp45_low.tif\n",
      "risk_class_2010-2039_CCSM4_rcp45_med.tif\n",
      "risk_class_2010-2039_CCSM4_rcp85_low.tif\n",
      "risk_class_2010-2039_CCSM4_rcp85_med.tif\n",
      "risk_class_2010-2039_GFDL-ESM2M_rcp45_low.tif\n",
      "risk_class_2010-2039_GFDL-ESM2M_rcp45_med.tif\n",
      "risk_class_2010-2039_GFDL-ESM2M_rcp85_low.tif\n",
      "risk_class_2010-2039_GFDL-ESM2M_rcp85_med.tif\n",
      "risk_class_2010-2039_HadGEM2-ES_rcp45_low.tif\n",
      "risk_class_2010-2039_HadGEM2-ES_rcp45_med.tif\n",
      "risk_class_2010-2039_HadGEM2-ES_rcp85_low.tif\n",
      "risk_class_2010-2039_HadGEM2-ES_rcp85_med.tif\n",
      "risk_class_2010-2039_MRI-CGCM3_rcp45_low.tif\n",
      "risk_class_2010-2039_MRI-CGCM3_rcp45_med.tif\n",
      "risk_class_2010-2039_MRI-CGCM3_rcp85_low.tif\n",
      "risk_class_2010-2039_MRI-CGCM3_rcp85_med.tif\n",
      "risk_class_2040-2069_CCSM4_rcp45_low.tif\n",
      "risk_class_2040-2069_CCSM4_rcp45_med.tif\n",
      "risk_class_2040-2069_CCSM4_rcp85_low.tif\n",
      "risk_class_2040-2069_CCSM4_rcp85_med.tif\n",
      "risk_class_2040-2069_GFDL-ESM2M_rcp45_low.tif\n",
      "risk_class_2040-2069_GFDL-ESM2M_rcp45_med.tif\n",
      "risk_class_2040-2069_GFDL-ESM2M_rcp85_low.tif\n",
      "risk_class_2040-2069_GFDL-ESM2M_rcp85_med.tif\n",
      "risk_class_2040-2069_HadGEM2-ES_rcp45_low.tif\n",
      "risk_class_2040-2069_HadGEM2-ES_rcp45_med.tif\n",
      "risk_class_2040-2069_HadGEM2-ES_rcp85_low.tif\n",
      "risk_class_2040-2069_HadGEM2-ES_rcp85_med.tif\n",
      "risk_class_2040-2069_MRI-CGCM3_rcp45_low.tif\n",
      "risk_class_2040-2069_MRI-CGCM3_rcp45_med.tif\n",
      "risk_class_2040-2069_MRI-CGCM3_rcp85_low.tif\n",
      "risk_class_2040-2069_MRI-CGCM3_rcp85_med.tif\n",
      "risk_class_2060-2099_CCSM4_rcp45_low.tif\n",
      "risk_class_2060-2099_CCSM4_rcp45_med.tif\n",
      "risk_class_2060-2099_CCSM4_rcp85_low.tif\n",
      "risk_class_2060-2099_CCSM4_rcp85_med.tif\n",
      "risk_class_2060-2099_GFDL-ESM2M_rcp45_low.tif\n",
      "risk_class_2060-2099_GFDL-ESM2M_rcp45_med.tif\n",
      "risk_class_2060-2099_GFDL-ESM2M_rcp85_low.tif\n",
      "risk_class_2060-2099_GFDL-ESM2M_rcp85_med.tif\n",
      "risk_class_2060-2099_HadGEM2-ES_rcp45_low.tif\n",
      "risk_class_2060-2099_HadGEM2-ES_rcp45_med.tif\n",
      "risk_class_2060-2099_HadGEM2-ES_rcp85_low.tif\n",
      "risk_class_2060-2099_HadGEM2-ES_rcp85_med.tif\n",
      "risk_class_2060-2099_MRI-CGCM3_rcp45_low.tif\n",
      "risk_class_2060-2099_MRI-CGCM3_rcp45_med.tif\n",
      "risk_class_2060-2099_MRI-CGCM3_rcp85_low.tif\n",
      "risk_class_2060-2099_MRI-CGCM3_rcp85_med.tif\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(subprocess.check_output([\"ls\", out_risk_dir]).decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
