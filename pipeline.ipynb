{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e00cc99",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AK spruce beetle outbreak risk pipeline\n",
    "\n",
    "This notebook is used for creating a climate-based dataset of spruce beetle outbreak risk.\n",
    "\n",
    "It is currently under development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84631a81",
   "metadata": {},
   "source": [
    "Define a function that returns a percentage survival based on univoltinism from supplied sequences of daily minimum and maximum temperatures for a given pixel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04fa0d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def univoltine(tmin, tmax):\n",
    "    try:\n",
    "        idx = np.where(tmax >= 16)[0][0]\n",
    "    except IndexError:\n",
    "        return 0\n",
    "    \n",
    "    tmin = tmin[idx + 40:idx + 90]\n",
    "    tmax = tmax[idx + 40:idx + 90]\n",
    "    # hour counter\n",
    "    k = 0\n",
    "    # easy if tmin ever above 17\n",
    "    # need to remember indices of values so we can exclude from\n",
    "    #  the hourly estimator\n",
    "    hot_idx = tmin > 17\n",
    "    k += 24 * hot_idx.sum()\n",
    "    # discard indices that counted for entire days above 17C\n",
    "    tmax = tmax[~hot_idx]\n",
    "    tmin = tmin[~hot_idx]\n",
    "    # need special treatment for tin == 17 as well, as it would\n",
    "    #  require division by zero in our estimation algorithm next. \n",
    "    #  just assume it is above 17 for 75% of the time, or 18 hrs\n",
    "    equal_idx = tmin == 17\n",
    "    k += 18 * equal_idx.sum()\n",
    "    # discard indices that counted for days where tmin == 17C\n",
    "    tmax = tmax[~equal_idx]\n",
    "    tmin = tmin[~equal_idx]\n",
    "    # then, multiply percent of temp difference above 17 by 24\n",
    "    #  to get estimate of hours above 17\n",
    "    # then get the estimate of remaining hours above 17 and add to\n",
    "    #  running total\n",
    "    h_est = ((tmax - 17) / (17 - tmin)) / 2 * 24\n",
    "    h_est[h_est < 0] = 0\n",
    "    k += h_est.sum()\n",
    "    \n",
    "    # then determine \"survival\" due to univoltinism\n",
    "    if k < 40:\n",
    "        x = 50\n",
    "    elif 40 <= k < 225:\n",
    "        x = 50 + (k - 40) / 14.8\n",
    "    elif 225 <= k < 412:\n",
    "        k = 62.5 + (k - 225) / 5\n",
    "    else:\n",
    "        k = 100\n",
    "\n",
    "    return round(k / 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b489f3",
   "metadata": {},
   "source": [
    "Define a function that returns a value representing percentage survival in the fall as a result of mortality caused by rapid cooling using daily temperature minimums for a given pixel / year. It should first compute the number of degree days below a threshold that decreases linearly at a rate of 0.5 C per day, within a 21 day window starting with the first day a minimum temperature of -12 is reached. This resulting \"degree days below the curve\" value will then be mapped to a percentage survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0335020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fall_survival(arr):\n",
    "    \"\"\"Execute the fall survival algorithm for an\n",
    "    array of temperature minimums for a single year.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        idx = np.where(arr <= -12)[0][0]\n",
    "    except IndexError:\n",
    "        return 1.0\n",
    "    \n",
    "    # return 0 if tmin is ever less than -30\n",
    "    if arr.min() < -30:\n",
    "        return 0\n",
    "    \n",
    "    window = arr[idx:idx + 21]\n",
    "    # cooling cutoff values\n",
    "    thr_arr = np.arange(-12, -22.5, -0.5)\n",
    "    dd = thr_arr - window\n",
    "    # count only positive values and sum\n",
    "    dd = dd[dd > 0].sum()\n",
    "    # ensure value is between 0 and 100\n",
    "    fall_survival = np.clip(100 - (dd * 4.76), 0, 100)\n",
    "        \n",
    "    return round(fall_survival / 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bffa18",
   "metadata": {},
   "source": [
    "Define a function that maps winter minimum temperature to percent survival. The survival rate will be based on snow cover, which will be one of three categories: minimal/no snow, moderate snowpack, deep snowpack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1f6f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def winter_survival(tmin, snow):\n",
    "    \"\"\"Map a supplied minimum temperature to percent survival\n",
    "    based on snowpack\n",
    "    \"\"\"\n",
    "    if snow == \"low\":\n",
    "        # linear ramp from -20 (100%) to -40 (0%) for no snowpack\n",
    "        winter_survival = 200 + 5*tmin\n",
    "    elif snow == \"med\":\n",
    "        # linear ramp from -30 (100%) to -50 (0%) for no snowpack\n",
    "        winter_survival = 250 + 5*tmin\n",
    "    elif snow == \"high\":\n",
    "        # linear ramp from -40 (100%) to -60 (0%) for no snowpack\n",
    "        winter_survival = 300 + 5*tmin\n",
    "    else:\n",
    "        raise ValueError(\"snow parameter must be one of low, med, or high\")\n",
    "    winter_survival = np.clip(winter_survival, 0, 100) \n",
    "\n",
    "    return np.round(winter_survival / 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85dc058",
   "metadata": {},
   "source": [
    "Define a function to compute the overall risk array for a given era, model, and scenario, using the above functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "631fe8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def compute_risk_arrays(args):\n",
    "    \"\"\"Compute the risk arrays from the NCAR BCSD data\n",
    "    for a given model, scenario, and era. Takes a single argument\n",
    "    for multiprocessing purposes. \n",
    "    \n",
    "    Args:\n",
    "        args (tuple): tuple of arguments of the form (met_dir, tmp_fn, era, model, scenario)\n",
    "        where arguments are defined as:\n",
    "            met_dir (pathlib.PosixPath): path to the directory containing met data organized as\n",
    "                folders named by model\n",
    "            tmp_fn (str): template filename string\n",
    "            era (str): era to be processed, of the form <start year>-<end year>\n",
    "            model (str): model to be processed\n",
    "            scenario (str): scenario to be processed\n",
    "    \n",
    "    Returns:\n",
    "        risk_da (xarray.DataArray): DataArray of risk with dimensions model, scenario,\n",
    "            snow load level, year, y index, x index\n",
    "    \"\"\"\n",
    "    met_dir, tmp_fn, era, model, scenario = args\n",
    "    yearly_risk_arrs = []\n",
    "    start_year, end_year = era.split(\"-\")\n",
    "    start_year = int(start_year)\n",
    "    end_year = int(end_year)\n",
    "    years = np.arange(start_year, end_year)\n",
    "    for year in years:\n",
    "        yr1_fp = met_dir.joinpath(model, scenario, tmp_fn.format(model, scenario, year - 1))\n",
    "        yr2_fp = met_dir.joinpath(model, scenario, tmp_fn.format(model, scenario, year))\n",
    "        with xr.open_mfdataset([yr1_fp, yr2_fp]) as ds:\n",
    "            winter_tmin = ds[\"tmin\"].sel(\n",
    "                time=slice(f\"{year - 1}-07-01\", f\"{year}-06-30\")\n",
    "            ).values\n",
    "            tmax = ds[\"tmax\"].sel(\n",
    "                time=slice(f\"{year}-01-01\", f\"{year}-12-31\")\n",
    "            ).values\n",
    "            tmin = ds[\"tmin\"].sel(\n",
    "                time=slice(f\"{year}-01-01\", f\"{year}-12-31\")\n",
    "            ).values\n",
    "        \n",
    "        survival = {}\n",
    "        survival[\"fall\"] = np.apply_along_axis(fall_survival, 0, winter_tmin)\n",
    "        # need to iterate over axes indices for summer \"survival\" because\n",
    "        #  both tmin and tmax arrays are needed\n",
    "        survival[\"summer\"] = np.empty(tmin.shape[1:])\n",
    "        for i, j in product(range(tmin.shape[1]), range(tmin.shape[2])):\n",
    "            survival[\"summer\"][i,j] = univoltine(tmin[:,i,j], tmax[:,i,j])\n",
    "        \n",
    "        # each year will have three risk arrays, one for each level of snowpack\n",
    "        year_risk_arr = []\n",
    "        snow_values = [\"low\", \"med\", \"high\"]\n",
    "        for snow in snow_values:\n",
    "            survival[\"winter\"] = winter_survival(winter_tmin.min(axis=0), snow)\n",
    "            year_risk_arr.append(\n",
    "                # just taking the raw product of all three \"survival\"\n",
    "                #  estimates for a yearly risk metric for now\n",
    "                np.prod(\n",
    "                    np.array(list(survival.values())), 0\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        yearly_risk_arrs.append(np.array(year_risk_arr))\n",
    "    \n",
    "    # flip along y axis because it's inverted in ingest data\n",
    "    yearly_risk = np.flip(np.array(yearly_risk_arrs), axis=2)\n",
    "    # swap the year and snow axes for more intuitive structure\n",
    "    #  to (snow, year, y, x) from (year, snow, y, x)\n",
    "    yearly_risk = np.swapaxes(yearly_risk, 0, 1)\n",
    "    # nodata_mask = np.broadcast_to(np.flipud(np.isnan(winter_tmin[0])), yearly_risk.shape)\n",
    "    # yearly_risk[nodata_mask] = np.nan\n",
    "    \n",
    "    # create a DataArray for easier construction of full DataArray with all results\n",
    "    risk_da = xr.DataArray(\n",
    "        # need to expand dims to add an extra for each of model, scenario\n",
    "        data=np.expand_dims(yearly_risk, (0, 1)),\n",
    "        dims=[\"model\", \"scenario\", \"snow\", \"year\", \"y\", \"x\"],\n",
    "        coords={\n",
    "            \"year\": ([\"year\"], years),\n",
    "            \"model\": ([\"model\"], [model]),\n",
    "            \"scenario\": ([\"scenario\"], [scenario]),\n",
    "            # need to flip lat/lon arrays as well, since the values are flipped above\n",
    "            \"longitude\": ([\"y\", \"x\"], np.flipud(ds[\"longitude\"].values)),\n",
    "            \"latitude\": ([\"y\", \"x\"], np.flipud(ds[\"latitude\"].values)),\n",
    "            \"snow\": ([\"snow\"], snow_values),\n",
    "        },\n",
    "        attrs=dict(\n",
    "            description=\"Climate-based beetle risk\",\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    return risk_da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677529ac",
   "metadata": {},
   "source": [
    "Process the datasets into risk arrays using `multiprocessing.Pool`. Set up all combinations of desired models / scenarios / eras as tasks.\n",
    "\n",
    "Set up path variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db789067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "met_dir = Path(\"/workspace/Shared/Tech_Projects/NCAR_AK/met\")\n",
    "tmp_fn = \"{}_{}_BCSD_met_{}.nc4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0332dc78-205a-4332-a2f3-7e1b88266480",
   "metadata": {},
   "source": [
    "Make arguments for `compute_risk_arrays` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39abc9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import luts\n",
    "\n",
    "\n",
    "args_list = []\n",
    "for model in luts.models:\n",
    "    for scenario in luts.scenarios:\n",
    "        for era in luts.eras:\n",
    "            # ignore hist + future eras\n",
    "            # if scenario == \"hist\" and era != \"1950-2009\":\n",
    "            #     continue\n",
    "            # simply ignore historical era /scenario for now\n",
    "            if scenario == \"hist\" or era == \"1950-2009\":\n",
    "                continue\n",
    "            args_list.append((met_dir, tmp_fn, era, model, scenario))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d46d8ba-154d-44a3-9af5-32303b35f301",
   "metadata": {},
   "source": [
    "Run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51f0ddda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [37:22<00:00, 56.05s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "\n",
    "\n",
    "# seems to be some limitation here for feasible number of workers.\n",
    "#  Perhaps memory limitation? Using 10 on Atlas 17, this completed in \n",
    "#  37 minutes (only the future scenarios being tested here)\n",
    "with Pool(10) as pool:\n",
    "    risk_das = [\n",
    "        result for result in tqdm.tqdm(\n",
    "            pool.imap_unordered(compute_risk_arrays, args_list), total=len(args_list))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8836598",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_da = xr.combine_by_coords(risk_das)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db428bf-e6b2-454c-97fa-75a425e73ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fp = \"/workspace/Shared/Tech_Projects/beetles/final_products/yearly_risk.nc\"\n",
    "\n",
    "risk_da.to_netcdf(out_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb23246-f8a3-48a9-92c4-089e4800950f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
