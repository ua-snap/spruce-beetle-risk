{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e00cc99",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AK spruce beetle outbreak risk pipeline\n",
    "\n",
    "This notebook constitutes the pipeline for producing a dataset of projected climate-driven risk of spruce beetle outbreak for forested areas of Alaska for the 21st century. See the [README](README.md) for more information.\n",
    "\n",
    "### Outputs\n",
    "\n",
    "The main product of this pipeline is a 5-D datacube of one categorical variable - climate-driven spruce beetle outbreak risk. The dimensions are:  \n",
    "\n",
    "* Era (time period)\n",
    "* Model\n",
    "* Scenario\n",
    "* Snowpack level\n",
    "* Y\n",
    "* X\n",
    "\n",
    "##### Format / structure\n",
    "\n",
    "This will be realized in typical SNAP / ARDAC fashion: a set of GeoTIFFs containing risk values for the entire spatial domain for a single realization of the first four dimension values, i.e. coordinates, and named according to those unique coordinate combinations.\n",
    "\n",
    "##### Spatial extent\n",
    "\n",
    "The expected spatial extent of the final dataset is the extent of the forest layer that the final risk data will be masked to. This will come from a version of the binary USFS \"Alaska Forest/Non-forest Map\" raster (found [here](https://data.fs.usda.gov/geodata/rastergateway/biomass/alaska_forest_nonforest.php)) in SNAP holdings that has been reprojected to EPSG:3338, found at `/workspace/Shared/Tech_Projects/beetles/project_data/ak_forest_mask.tif`.\n",
    "\n",
    "##### Temporal extent\n",
    "\n",
    "The risk values will be computed for 30-year long eras of the 21st century:  \n",
    "* 2010-2039\n",
    "* 2040-2069\n",
    "* 2070-2099\n",
    "\n",
    "### Base data\n",
    "\n",
    "The base / input data used for computing the climate-driven risk of beetle outbreaks is the \"[21st Century Hydrologic Projections for Alaska and Hawaii](https://www.earthsystemgrid.org/dataset/ucar.ral.hydro.predictions.html)\" dataset produced by NCAR, specifically the \"Alaska Near Surface Meteorology Daily Averages\" child dataset. This dataset is available on SNAP infra at `/Data/Base_Data/Climate/AK_NCAR_12km/met`.\n",
    "\n",
    "## Pipeline steps\n",
    "\n",
    "0. Setup - Set up path variables, slurm variables, directories, intial conditions, etc.\n",
    "1. Process yearly risk and risk components\n",
    "2. Process the final risk class dataset\n",
    "\n",
    "## 0 - Setup\n",
    "\n",
    "Sets up path variables, slurm variables, directories, intial conditions, imports, etc. Execute this cell before any other step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db789067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d461564-583e-42a8-9f8d-7b0eb263e937",
   "metadata": {},
   "source": [
    "## 1 - Process yearly risk and risk components\n",
    "\n",
    "This section creates the yearly risk dataset - a collection of risk values for each year across the grid. This dataset is not expected to be the final product, but it could be a useful intermediate product.\n",
    "\n",
    "The yearly risk values are calculated from three yearly \"risk components\". Saving these components as a dataset may have some merit on its own, at least for validation if nothing else. This step utilizes slurm to handle execution of the `compute_yearly_risk.py` script on all model/scenario combinations. \n",
    "\n",
    "We will process all future years for the expected final summary time periods: 2008-2099. We will process all years available for the Daymet dataset as well: 1980-2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2929f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"slurm_email\": slurm_email,\n",
    "    \"partition\": partition,\n",
    "    \"conda_init_script\": conda_init_script,\n",
    "    \"ap_env\": ap_env,\n",
    "    \"risk_script\": risk_script,\n",
    "    \"met_dir\": met_dir,\n",
    "    # template filename for NCAR met data\n",
    "    \"tmp_fn\": \"{}_{}_BCSD_met_{}.nc4\",\n",
    "}\n",
    "\n",
    "sbatch_fps = []\n",
    "for model in luts.models:\n",
    "    for scenario in luts.scenarios:\n",
    "        sbatch_fp, sbatch_out_fp = slurm.get_yearly_fps(slurm_dir, model, luts.full_future_era, scenario)\n",
    "        risk_comp_fp = risk_comp_dir.joinpath(f\"risk_components_{model}_{scenario}_{luts.full_future_era}.nc\")\n",
    "        yearly_risk_fp = yearly_risk_dir.joinpath(f\"yearly_risk_{model}_{scenario}_{luts.full_future_era}.nc\")\n",
    "        \n",
    "        kwargs.update({\n",
    "            \"sbatch_fp\": sbatch_fp,\n",
    "            \"sbatch_out_fp\": sbatch_out_fp,\n",
    "            \"risk_comp_fp\": risk_comp_fp,\n",
    "            \"yearly_risk_fp\": yearly_risk_fp,\n",
    "            \"era\": luts.full_future_era,\n",
    "            \"model\": model,\n",
    "            \"scenario\": scenario,\n",
    "        })\n",
    "\n",
    "        slurm.write_sbatch_yearly_risk(**kwargs)\n",
    "        sbatch_fps.append(sbatch_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c18164b",
   "metadata": {},
   "source": [
    "We also have the daymet dataset that needs to be processed using different years from all of the projected data. Create an sbatch job for that, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9555f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"daymet\"\n",
    "era = \"1980-2017\"\n",
    "sbatch_fp, sbatch_out_fp = slurm.get_yearly_fps(slurm_dir, model, era)\n",
    "risk_comp_fp = risk_comp_dir.joinpath(f\"risk_components_{model}_{era}.nc\")\n",
    "yearly_risk_fp = yearly_risk_dir.joinpath(f\"yearly_risk_{model}_{era}.nc\")\n",
    "\n",
    "kwargs.update({\n",
    "    \"sbatch_fp\": sbatch_fp,\n",
    "    \"sbatch_out_fp\": sbatch_out_fp,\n",
    "    \"met_dir\": met_dir,\n",
    "    \"tmp_fn\": \"daymet_met_{}.nc\",\n",
    "    \"risk_comp_fp\": risk_comp_fp,\n",
    "    \"yearly_risk_fp\": yearly_risk_fp,\n",
    "    \"era\": era,\n",
    "    \"model\": model,\n",
    "    \"scenario\": None,\n",
    "})\n",
    "\n",
    "slurm.write_sbatch_yearly_risk(**kwargs)\n",
    "sbatch_fps.append(sbatch_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2af05b",
   "metadata": {},
   "source": [
    "Remove existing slurm output files if desired:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e088ea4f-93f1-42d1-a169-e16db4b9bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove existing output files if desired\n",
    "_ = [fp.unlink() for fp in slurm_dir.glob(\"*.out\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fd5dc7",
   "metadata": {},
   "source": [
    "Submit the sbatch jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20db0341",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ids = [slurm.submit_sbatch(fp) for fp in sbatch_fps]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107d0d66",
   "metadata": {},
   "source": [
    "## 2 - Process the final risk class dataset\n",
    "\n",
    "Process the yearly risk data into risk classes for the three future eras. Since this doesn't take very long, we can process in the notebook instead of slurming it. This will involve two steps:\n",
    "\n",
    "1. Preparing files for masking to forested area of Alaska\n",
    "2. Classifying risk and saving masked dataset\n",
    "\n",
    "### 2.1 - Prepare files for masking risk class dataset\n",
    "\n",
    "We want the final risk class dataset to be masked to the forested areas of Alaska, so there is some prep work that needs to happen first:\n",
    "\n",
    "1. Georeference the NCAR grid and save it for a template for regridding the forest mask to\n",
    "2. Re-grid the forest mask (~250m resolution) to match the NCAR template\n",
    "\n",
    "Completing those steps will give a forest mask that is on the same grid as the NCAR data which can be easily used for masking the final risk class dataset.\n",
    "\n",
    "#### 2.1.1 Georeference NCAR grid\n",
    "\n",
    "The NCAR data files have only the latitude and longitude geogrids defining the centerpoints of each pixel in the grid - no other spatial information. This is therefore also the case for our new risk components and yearly risk datasets. \n",
    "\n",
    "To mask our grid to the forested area of Alaska, we want a forest mask raster that is on the same grid as our new datasets. So we want to create a GeoTIFF file for the NCAR grid as a template.\n",
    "\n",
    "Read in one of the time slices of an NCAR file to get the grid (i.e. just the 2-D array of data), derive the projection info using some info provided by NCAR about this dataset, and create a GeoTIFF template file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "883fbe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open an NCAR file to get some info from\n",
    "with xr.open_dataset(met_dir.joinpath(\"daymet/daymet_met_1980.nc\")) as ds:\n",
    "    # need grid shape below\n",
    "    ny, nx = ds.longitude.shape\n",
    "    ncar_arr = np.flipud(ds[\"tmin\"].values[0])\n",
    "\n",
    "# values provided by NCAR (via email correspondence)\n",
    "wrf_proj_str = PolarStereographic(**{\"TRUELAT1\": 64, \"STAND_LON\": -150}).proj4()\n",
    "wrf_proj = Proj(wrf_proj_str)\n",
    "wgs_proj = Proj(proj='latlong', datum='WGS84')\n",
    "transformer = Transformer.from_proj(wgs_proj, wrf_proj)\n",
    "e, n = transformer.transform(-150, 64)\n",
    "# Grid parameters\n",
    "dx, dy = 12000, 12000\n",
    "# Down left corner of the domain\n",
    "x0 = -(nx-1) / 2. * dx + e\n",
    "y0 = -(ny-1) / 2. * dy + n\n",
    "# 2d grid\n",
    "x = np.arange(nx) * dx + x0\n",
    "y = np.arange(ny) * dy + y0\n",
    "\n",
    "# these coordinates will be used here and for spatially\n",
    "#  referencing all resulting risk class data files\n",
    "ncar_coords = {\n",
    "    \"y\": ([\"y\"], np.flip(y)),\n",
    "    \"x\": ([\"x\"], x),\n",
    "}\n",
    "\n",
    "da = xr.DataArray(\n",
    "    data=ncar_arr,\n",
    "    dims=[\"y\", \"x\"],\n",
    "    coords=ncar_coords,\n",
    ")\n",
    "da.attrs[\"_FillValue\"] = np.nan\n",
    "\n",
    "temp_ncar_fp = scratch_dir.joinpath(\"ncar_template_3338.tif\")\n",
    "da.rio.write_crs(wrf_proj_str).rio.reproject(\"EPSG:3338\").rio.to_raster(temp_ncar_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995291dc",
   "metadata": {},
   "source": [
    "#### 2.1.2 - Regrid the forest mask\n",
    "\n",
    "Now regrid the forest mask to match the new NCAR template. \n",
    "\n",
    "Since the NCAR data has a larger extent than the forest mask, we will clip (crop) the template file to the extent of the forest mask before regridding the mask.\n",
    "\n",
    "Create a shapefile to clip to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f185680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new index file...\n"
     ]
    }
   ],
   "source": [
    "cut_fp = scratch_dir.joinpath(\"clip_ncar.shp\")\n",
    "cut_fp.unlink(missing_ok=True)\n",
    "_ = subprocess.call([\"gdaltindex\", cut_fp, forest_fp])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aab9dda",
   "metadata": {},
   "source": [
    "Then clip the template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4676f921",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ncar_clip_fp = scratch_dir.joinpath(\"ncar_template_clipped_3338.tif\")\n",
    "temp_ncar_clip_fp.unlink(missing_ok=True)\n",
    "_ = subprocess.call(\n",
    "    [\n",
    "        \"gdalwarp\",\n",
    "        \"-cutline\",\n",
    "        cut_fp,\n",
    "        \"-crop_to_cutline\",\n",
    "        \"-q\",\n",
    "        \"-overwrite\",\n",
    "        temp_ncar_fp,\n",
    "        temp_ncar_clip_fp,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bd6816",
   "metadata": {},
   "source": [
    "Then get the new metadata from the clipped NCAR file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3617b197",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rio.open(temp_ncar_clip_fp) as src:\n",
    "    temp_meta = src.meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a9ddfe",
   "metadata": {},
   "source": [
    "Update the data type and nodata value to match that of existing mask: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a2ca2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_meta.update({\"dtype\": \"uint8\", \"nodata\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a76d2",
   "metadata": {},
   "source": [
    "Write a blank array with this metadata to a new GeoTIFF that will serve as a target grid for the original forest mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7762e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_arr = np.zeros((1, temp_meta[\"height\"], temp_meta[\"width\"]), dtype=\"uint8\")\n",
    "\n",
    "ncar_forest_fp = scratch_dir.joinpath(\"ak_forest_mask_ncar_3338.tif\")\n",
    "ncar_forest_fp.unlink(missing_ok=True)\n",
    "with rio.open(ncar_forest_fp, \"w\", **temp_meta) as src:\n",
    "    src.write(temp_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee48763",
   "metadata": {},
   "source": [
    "Now regrid the original forest mask by calling `gadalwarp` on it with this new target GeoTIFF as the output file. The blank data (all 0's) of the target file will be updated to match the original forest mask file, effectively regridding that data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ad33c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = subprocess.call([\"gdalwarp\", \"-q\", forest_fp, ncar_forest_fp])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ab65b8",
   "metadata": {},
   "source": [
    "### 2.2 - Classify risk and mask\n",
    "\n",
    "Using our new forest mask and template NCAR file for clipping, we will classify risk, clip, and mask all output GeoTIFFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "03c7c150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/workspace/UA/kmredilla/spruce-beetle-risk/utils.py'>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a941cf50",
   "metadata": {},
   "source": [
    "Remember to wait until the above slurm jobs have finished so we have all of the yearly risk data files to work with. You can check if any are still running or queued with this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6aaa5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slurm.jobs_running(job_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d817eb2",
   "metadata": {},
   "source": [
    "when the above function returns `False`, or when you have otherwise verified that the yearly risk dataset is completed, then iterate over the models / scenarios / snow levels / future eras, and classify, clip, and mask: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cb8c78eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = product(luts.models, luts.scenarios, [\"low\", \"med\"], luts.eras)\n",
    "\n",
    "kwargs = {\n",
    "    \"ncar_coords\": ncar_coords,\n",
    "    \"wrf_proj_str\": wrf_proj_str,\n",
    "    \"cut_fp\": cut_fp,\n",
    "    \"ncar_forest_fp\": ncar_forest_fp,\n",
    "}\n",
    "\n",
    "for model, scenario, snow, era in args:\n",
    "    yearly_risk_fp = yearly_risk_dir.joinpath(f\"yearly_risk_{model}_{scenario}_{luts.full_future_era}.nc\")\n",
    "    for snow in [\"low\", \"med\"]:\n",
    "        risk_class_fp = risk_class_dir.joinpath(f\"risk_class_{era}_{model}_{scenario}_{snow}.tif\")\n",
    "        # makes things a little more striaghtfroward\n",
    "        kwargs.update({\n",
    "            \"yearly_risk_fp\": yearly_risk_fp,\n",
    "            \"era\": era,\n",
    "            \"snow\": snow,\n",
    "            \"risk_class_fp\": risk_class_fp,\n",
    "        })\n",
    "        utils.run_classify_clip_mask(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a31c4c",
   "metadata": {},
   "source": [
    "Also do the same for the historical era of the Daymet-based yearly risk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "552bafa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "daymet_era = \"1982-2017\"\n",
    "yearly_risk_fp = yearly_risk_dir.joinpath(f\"yearly_risk_daymet_1980-2017.nc\")\n",
    "for snow in [\"low\", \"med\"]:\n",
    "    risk_class_fp = risk_class_dir.joinpath(f\"risk_class_{daymet_era}_daymet_hist_{snow}.tif\")\n",
    "    kwargs.update({\n",
    "        \"yearly_risk_fp\": yearly_risk_fp,\n",
    "        \"era\": era,\n",
    "        \"snow\": snow,\n",
    "        \"risk_class_fp\": risk_class_fp,\n",
    "    })\n",
    "    utils.run_classify_clip_mask(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100df840",
   "metadata": {},
   "source": [
    "And copy these files to `$OUTPUT_DIR` for safe-keeping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "becc0a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_args = [(fp, out_risk_dir.joinpath(fp.name)) for fp in risk_class_dir.glob(\"*.tif\")]\n",
    "_ = [shutil.copy(*arg) for arg in copy_args]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9664581a",
   "metadata": {},
   "source": [
    "## Pipeline end!\n",
    "\n",
    "That's it! Beetle risk secured:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a72a6c2d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2600\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_1982-2017_daymet_hist_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_1982-2017_daymet_hist_med.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2010-2039_CCSM4_rcp45_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2010-2039_CCSM4_rcp45_med.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2010-2039_CCSM4_rcp85_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2010-2039_CCSM4_rcp85_med.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2010-2039_GFDL-ESM2M_rcp45_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2010-2039_GFDL-ESM2M_rcp45_med.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2010-2039_GFDL-ESM2M_rcp85_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2010-2039_GFDL-ESM2M_rcp85_med.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2010-2039_HadGEM2-ES_rcp45_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2010-2039_HadGEM2-ES_rcp45_med.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2010-2039_HadGEM2-ES_rcp85_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2010-2039_HadGEM2-ES_rcp85_med.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2010-2039_MRI-CGCM3_rcp45_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2010-2039_MRI-CGCM3_rcp45_med.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2010-2039_MRI-CGCM3_rcp85_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2010-2039_MRI-CGCM3_rcp85_med.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2040-2069_CCSM4_rcp45_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2040-2069_CCSM4_rcp45_med.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2040-2069_CCSM4_rcp85_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2040-2069_CCSM4_rcp85_med.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2040-2069_GFDL-ESM2M_rcp45_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2040-2069_GFDL-ESM2M_rcp45_med.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2040-2069_GFDL-ESM2M_rcp85_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2040-2069_GFDL-ESM2M_rcp85_med.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2040-2069_HadGEM2-ES_rcp45_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2040-2069_HadGEM2-ES_rcp45_med.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2040-2069_HadGEM2-ES_rcp85_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2040-2069_HadGEM2-ES_rcp85_med.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2040-2069_MRI-CGCM3_rcp45_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2040-2069_MRI-CGCM3_rcp45_med.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2040-2069_MRI-CGCM3_rcp85_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2040-2069_MRI-CGCM3_rcp85_med.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2060-2099_CCSM4_rcp45_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2060-2099_CCSM4_rcp45_med.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2060-2099_CCSM4_rcp85_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2060-2099_CCSM4_rcp85_med.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2060-2099_GFDL-ESM2M_rcp45_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2060-2099_GFDL-ESM2M_rcp45_med.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2060-2099_GFDL-ESM2M_rcp85_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2060-2099_GFDL-ESM2M_rcp85_med.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2060-2099_HadGEM2-ES_rcp45_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2060-2099_HadGEM2-ES_rcp45_med.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2060-2099_HadGEM2-ES_rcp85_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2060-2099_HadGEM2-ES_rcp85_med.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2060-2099_MRI-CGCM3_rcp45_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2060-2099_MRI-CGCM3_rcp45_med.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2060-2099_MRI-CGCM3_rcp85_low.tif\n",
      "-rw-r--r--. 1 kmredilla snap_users 45069 Sep 16 18:14 risk_class_2060-2099_MRI-CGCM3_rcp85_med.tif\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(subprocess.check_output([\"ls\", \"-l\", out_risk_dir]).decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7a1d04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
