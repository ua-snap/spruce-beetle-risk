{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e0a0249-3265-4621-9870-353d9195f908",
   "metadata": {},
   "source": [
    "# Model Calibration\n",
    "\n",
    "This notebook is for calibrating the model. It extracts data from the yearly risk dataset created in `./pipeline.ipynb` for locations and years we have observed data on, and provides other visualization utilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29608210-4ee2-4d1d-bfd7-06a97317f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# paths\n",
    "out_dir = Path(\"/workspace/Shared/Tech_Projects/beetles/final_products\")\n",
    "yearly_risk_fp = out_dir.joinpath(\"yearly_risk_daymet.nc\")\n",
    "scratch_dir = Path(\"/atlas_scratch/kmredilla/beetles\")\n",
    "daymet_comp_fp = scratch_dir.joinpath(\"yearly_risk_components_daymet.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2032d5d7-6ab1-43d3-a469-c304f7e864e8",
   "metadata": {},
   "source": [
    "Define the locations and years we are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b2128d1-6b6c-4a46-ac09-2b66b4ee60e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "locations_lu = {\n",
    "    \"Talkeetna\": {\n",
    "        \"years\": list(range(2012, 2018)),\n",
    "        \"latlon\": (62.3209, -150.1066),\n",
    "    },\n",
    "    \"Fairbanks\": {\n",
    "        \"years\": list(range(2014, 2020)),\n",
    "        \"latlon\": (64.8401, -147.7200),\n",
    "    },\n",
    "    \"King Salmon\": {\n",
    "        \"years\": list(range(2012, 2019)),\n",
    "        \"latlon\": (58.6887, -156.6628),\n",
    "    },\n",
    "    \"Delta Jct\": {\n",
    "        \"years\": list(range(2014, 2020)),\n",
    "        \"latlon\": (64.0401, -145.7344),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d133b74c-c969-4995-a5d0-6d099cb0b14b",
   "metadata": {},
   "source": [
    "Define functions to extract the risk values from the risk dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31e5fb25-5fe5-4457-a1a8-c4c57c7520aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latlon_to_idx(da, lat, lon):\n",
    "    if da[\"longitude\"].values.max() > 0:\n",
    "        # create new longitude array for point extraction that \n",
    "        #  is on the [-360, 0) scale in case it's not\n",
    "        new_lon = da[\"longitude\"].values.copy()\n",
    "        new_lon[new_lon > 0] = new_lon[new_lon > 0] - 360\n",
    "    else:\n",
    "        new_lon = ds[\"longitude\"].values\n",
    "    \n",
    "    dist_arr = np.sqrt(\n",
    "        np.square(new_lon - lon) + np.square(da[\"latitude\"] - lat)\n",
    "    ).values\n",
    "\n",
    "    yidx, xidx = np.where(dist_arr == dist_arr.min())\n",
    "    \n",
    "    return yidx, xidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5ce4bf7-1cbe-4909-a842-46f4f30fe79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_risk_df(da, lat, lon, years):\n",
    "    \"\"\"Extract a dataframe of risk values for a given location\"\"\"\n",
    "    yidx, xidx = latlon_to_idx(da, lat, lon)\n",
    "    risk_df = da.sel(y=yidx, x=xidx, year=years).drop([\"latitude\", \"longitude\"]).to_dataframe(\n",
    "        \"risk\"\n",
    "    ).reset_index().drop(columns=[\"x\", \"y\"])\n",
    "    \n",
    "    return risk_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a9bf66-3775-498b-a8a3-7500189d62a7",
   "metadata": {},
   "source": [
    "Define a function to extract the risk components involvied in a year's risk computation for a given location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c56e5f4-ec7b-4ae6-adc7-6b3848bfaf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_components(daymet_comp_fp, year, snow, lat, lon):\n",
    "    \n",
    "    components = {}\n",
    "    with xr.open_dataset(daymet_comp_fp) as comp_ds:\n",
    "        yidx, xidx = latlon_to_idx(comp_ds, lat, lon)\n",
    "        const_args = {\"model\": \"daymet\", \"scenario\": None, \"y\": yidx, \"x\": xidx}\n",
    "        \n",
    "        components[\"u_t2\"] = comp_ds[\"summer_survival\"].sel(year=(year - 2), **const_args).values[0][0]\n",
    "        components[\"u_t1\"] = comp_ds[\"summer_survival\"].sel(year=(year - 1), **const_args).values[0][0]\n",
    "        # \"not univoltine\"\n",
    "        components[\"un_t2\"] = np.round(1 - components[\"u_t2\"], 2)\n",
    "        components[\"x2_t2\"] = comp_ds[\"fall_survival\"].sel(year=(year - 2), **const_args).values[0][0]\n",
    "        components[\"x2_t1\"] = comp_ds[\"fall_survival\"].sel(year=(year - 1), **const_args).values[0][0]\n",
    "        components[\"x3_t2\"] = comp_ds[\"winter_survival\"].sel(year=(year - 2), snow=snow, **const_args).values[0][0]\n",
    "        components[\"x3_t1\"] = comp_ds[\"winter_survival\"].sel(year=(year - 1), snow=snow, **const_args).values[0][0]\n",
    "\n",
    "    return components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a39db26-5375-4292-9e2e-7b2e90e6e328",
   "metadata": {},
   "source": [
    "Apply the function and create a complete dataframe of extracted values for all desired locations and years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f06cb17f-84e8-4fcc-937a-7312a3b19c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "with xr.open_dataset(yearly_risk_fp) as risk_ds:\n",
    "    risk_df_list = []\n",
    "    # start of iteration over locations\n",
    "    for location in locations_lu:\n",
    "        years = locations_lu[location][\"years\"]\n",
    "        lat, lon = locations_lu[location][\"latlon\"]\n",
    "        # ensure years are limited to daymet availability (up to 2017)\n",
    "        years = [year for year in years if year <= 2017]\n",
    "        temp_df = extract_risk_df(risk_ds[\"risk\"], lat, lon, years)\n",
    "        temp_df[\"location\"] = location\n",
    "        \n",
    "        temp_components = []\n",
    "        for idx, row in temp_df.iterrows():\n",
    "            snow = row[\"snow\"]\n",
    "            year = row[\"year\"]\n",
    "            temp_components.append(get_components(daymet_comp_fp, year, snow, lat, lon))\n",
    "        temp_df[\"components\"] = temp_components\n",
    "        \n",
    "        risk_df_list.append(temp_df)\n",
    "\n",
    "risk_df = pd.concat(risk_df_list)\n",
    "risk_df = risk_df[[\"location\"] + list(risk_df.columns[:-2]) + [\"components\"]]\n",
    "\n",
    "comp_df = risk_df[\"components\"].apply(pd.Series)\n",
    "risk_df = risk_df.drop(columns=\"components\")\n",
    "risk_df[comp_df.columns] = comp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fce3eaa-5c51-41a1-a7d8-48c686b888f6",
   "metadata": {},
   "source": [
    "Write the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45b8cdd7-7b4c-455e-8595-b3d182f309ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for comp in [\"u_t2\", \"u_t1\", \"un_t2\", \"x2_t2\", \"x2_t1\", \"x3_t2\", \"x3_t1\"]:\n",
    "    risk_df[comp] = risk_df[comp].map(lambda x: '%.2f' % x)\n",
    "risk_table_fp = out_dir.joinpath(\"daymet_risk_extraction.csv\")\n",
    "risk_df.to_csv(risk_table_fp, index=False, float_format='%.6f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98c8a73-79db-49c5-a49f-96c36f09adc9",
   "metadata": {},
   "source": [
    "Temp code for copying this CSV to Google Drive:\n",
    "\n",
    "```\n",
    "rclone copy /workspace/Shared/Tech_Projects/beetles/final_products/daymet_risk_extraction.csv google-drive:Beetles/data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74235396-dc33-4960-9089-d9622f608f52",
   "metadata": {},
   "source": [
    "## Profiling\n",
    "\n",
    "Set up some sort of structure for profiling the weights, i.e. for re-processing the risk dataset with different weights programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "35931e0b-1491-41ae-9a32-d9d9aa924416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd8cc5-178e-4b85-aa84-0f2218f75975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
