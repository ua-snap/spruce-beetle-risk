{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e0a0249-3265-4621-9870-353d9195f908",
   "metadata": {},
   "source": [
    "# Model Calibration\n",
    "\n",
    "This notebook was made to help calibrate the model. It is being retained because its outputs may be important.\n",
    "\n",
    "Run this cell before running any other parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29608210-4ee2-4d1d-bfd7-06a97317f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2032d5d7-6ab1-43d3-a469-c304f7e864e8",
   "metadata": {},
   "source": [
    "## Risk component daymet point extractions\n",
    "\n",
    "This section will extract the daymet-derived risk components for a subset of locations.\n",
    "\n",
    "Define the locations and years we are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b2128d1-6b6c-4a46-ac09-2b66b4ee60e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "years = list(range(2010, 2018))\n",
    "locations_lu = {\n",
    "    \"Talkeetna\": {\n",
    "        \"years\": years,\n",
    "        \"latlon\": (62.3209, -150.1066),\n",
    "    },\n",
    "    \"Fairbanks\": {\n",
    "        \"years\": years,\n",
    "        \"latlon\": (64.8401, -147.7200),\n",
    "    },\n",
    "    \"King Salmon\": {\n",
    "        \"years\": years,\n",
    "        \"latlon\": (58.6887, -156.6628),\n",
    "    },\n",
    "    \"Delta Jct\": {\n",
    "        \"years\": years,\n",
    "        \"latlon\": (64.0401, -145.7344),\n",
    "    },\n",
    "    \n",
    "    \"Wasilla\": {\n",
    "        \"years\": years,\n",
    "        \"latlon\": (61.5809, -149.4411),\n",
    "    },\n",
    "    \"Fort Yukon\": {\n",
    "        \"years\": years,\n",
    "        \"latlon\": (66.5637, -145.2487),\n",
    "    },\n",
    "    \"Tok\": {\n",
    "        \"years\": years,\n",
    "        \"latlon\": (63.3359, -142.9877),\n",
    "    },\n",
    "    \"Glenallen\": {\n",
    "        \"years\": years,\n",
    "        \"latlon\": (62.1081, -145.5340),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d133b74c-c969-4995-a5d0-6d099cb0b14b",
   "metadata": {},
   "source": [
    "Define functions to extract the risk values from the yearly risk dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31e5fb25-5fe5-4457-a1a8-c4c57c7520aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latlon_to_idx(da, lat, lon):\n",
    "    if da[\"longitude\"].values.max() > 0:\n",
    "        # create new longitude array for point extraction that \n",
    "        #  is on the [-360, 0) scale in case it's not\n",
    "        new_lon = da[\"longitude\"].values.copy()\n",
    "        new_lon[new_lon > 0] = new_lon[new_lon > 0] - 360\n",
    "    else:\n",
    "        new_lon = ds[\"longitude\"].values\n",
    "    \n",
    "    dist_arr = np.sqrt(\n",
    "        np.square(new_lon - lon) + np.square(da[\"latitude\"] - lat)\n",
    "    ).values\n",
    "\n",
    "    yidx, xidx = np.where(dist_arr == dist_arr.min())\n",
    "    \n",
    "    return yidx, xidx\n",
    "\n",
    "\n",
    "def extract_risk_df(da, lat, lon, years):\n",
    "    \"\"\"Extract a dataframe of risk values for a given location\"\"\"\n",
    "    yidx, xidx = latlon_to_idx(da, lat, lon)\n",
    "    risk_df = da.sel(y=yidx, x=xidx, year=years).drop([\"latitude\", \"longitude\"]).to_dataframe(\n",
    "        \"risk\"\n",
    "    ).reset_index().drop(columns=[\"x\", \"y\"])\n",
    "    \n",
    "    return risk_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a9bf66-3775-498b-a8a3-7500189d62a7",
   "metadata": {},
   "source": [
    "Define a function to extract the risk components involvied in a year's risk computation for a given location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c56e5f4-ec7b-4ae6-adc7-6b3848bfaf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_components(daymet_comp_fp, year, snow, lat, lon):\n",
    "    components = {}\n",
    "    with xr.open_dataset(daymet_comp_fp) as comp_ds:\n",
    "        yidx, xidx = latlon_to_idx(comp_ds, lat, lon)\n",
    "        const_args = {\"y\": yidx, \"x\": xidx}\n",
    "        \n",
    "        components[\"u_t2\"] = comp_ds[\"summer_survival\"].sel(year=(year - 2), **const_args).values[0][0]\n",
    "        components[\"u_t1\"] = comp_ds[\"summer_survival\"].sel(year=(year - 1), **const_args).values[0][0]\n",
    "        # \"not univoltine\"\n",
    "        components[\"un_t2\"] = np.round(1 - components[\"u_t2\"], 2)\n",
    "        components[\"x2_t2\"] = comp_ds[\"fall_survival\"].sel(year=(year - 2), **const_args).values[0][0]\n",
    "        components[\"x2_t1\"] = comp_ds[\"fall_survival\"].sel(year=(year - 1), **const_args).values[0][0]\n",
    "        components[\"x3_t2\"] = comp_ds[\"winter_survival\"].sel(year=(year - 2), snow=snow, **const_args).values[0][0]\n",
    "        components[\"x3_t1\"] = comp_ds[\"winter_survival\"].sel(year=(year - 1), snow=snow, **const_args).values[0][0]\n",
    "\n",
    "    return components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a39db26-5375-4292-9e2e-7b2e90e6e328",
   "metadata": {},
   "source": [
    "Apply the functions and create a complete dataframe of extracted values for all desired locations and years for the daymet risk components and yearly risk datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f06cb17f-84e8-4fcc-937a-7312a3b19c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_risk_fp = yearly_risk_dir.joinpath(\"yearly_risk_daymet_1980-2017.nc\")\n",
    "daymet_comp_fp = risk_comp_dir.joinpath(\"risk_components_daymet_1980-2017.nc\")\n",
    "with xr.open_dataset(yearly_risk_fp) as risk_ds:\n",
    "    risk_df_list = []\n",
    "    # start of iteration over locations\n",
    "    for location in locations_lu:\n",
    "        years = locations_lu[location][\"years\"]\n",
    "        lat, lon = locations_lu[location][\"latlon\"]\n",
    "        # ensure years are limited to daymet availability (up to 2017)\n",
    "        years = [year for year in years if year <= 2017]\n",
    "        temp_df = extract_risk_df(risk_ds[\"risk\"], lat, lon, years)\n",
    "        temp_df[\"location\"] = location\n",
    "        \n",
    "        temp_components = []\n",
    "        for idx, row in temp_df.iterrows():\n",
    "            snow = row[\"snow\"]\n",
    "            year = row[\"year\"]\n",
    "            temp_components.append(get_components(daymet_comp_fp, year, snow, lat, lon))\n",
    "        temp_df[\"components\"] = temp_components\n",
    "        \n",
    "        risk_df_list.append(temp_df)\n",
    "\n",
    "risk_df = pd.concat(risk_df_list)\n",
    "risk_df = risk_df[[\"location\"] + list(risk_df.columns[:-2]) + [\"components\"]]\n",
    "\n",
    "comp_df = risk_df[\"components\"].apply(pd.Series)\n",
    "risk_df = risk_df.drop(columns=\"components\")\n",
    "risk_df[comp_df.columns] = comp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fce3eaa-5c51-41a1-a7d8-48c686b888f6",
   "metadata": {},
   "source": [
    "Write the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45b8cdd7-7b4c-455e-8595-b3d182f309ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for comp in [\"u_t2\", \"u_t1\", \"un_t2\", \"x2_t2\", \"x2_t1\", \"x3_t2\", \"x3_t1\"]:\n",
    "    risk_df[comp] = risk_df[comp].map(lambda x: '%.2f' % x)\n",
    "risk_table_fp = scratch_dir.joinpath(\"daymet_risk_extraction.csv\")\n",
    "risk_df.to_csv(risk_table_fp, index=False, float_format='%.6f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a2837",
   "metadata": {},
   "source": [
    "## Daymet yearly risk maps\n",
    "\n",
    "Create risk maps for all years of Daymet data using the yearly risk dataset.\n",
    "\n",
    "Load the Daymet yearly risk data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50592384",
   "metadata": {},
   "outputs": [],
   "source": [
    "daymet_risk_fp = yearly_risk_dir.joinpath(\"yearly_risk_daymet_1980-2017.nc\")\n",
    "ds = xr.load_dataset(daymet_risk_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edf1932",
   "metadata": {},
   "source": [
    "Create output directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f931770",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = scratch_dir.joinpath(\"daymet_risk_maps\")\n",
    "out_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecf5fb2",
   "metadata": {},
   "source": [
    "Iterate, create the maps, and save them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39da42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(1982, 2018):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(18, 8))\n",
    "    arr = ds[\"risk\"].sel(snow=\"med\", year=year).values\n",
    "    im = ax.imshow(arr, interpolation=\"none\")\n",
    "    plt.axis(\"off\")\n",
    "    cbar_ax = fig.add_axes([0.7, 0.15, 0.02, 0.7])\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "    cbar.ax.get_yaxis().labelpad = 30\n",
    "    cbar.set_label(\"Beetle\\nrisk\", size=14, rotation=0)\n",
    "    ax.set_title(f\"Beetle risk (Daymet), {year}\", size=14, pad=-5)\n",
    "    out_fp = out_dir.joinpath(f\"daymet_risk_{year}.png\")\n",
    "    plt.savefig(out_fp, bbox_inches='tight', facecolor=\"white\")\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
